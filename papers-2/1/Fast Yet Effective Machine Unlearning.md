>  Unlearning multiple classes requires a similar number of update steps as for a single class, making our approach scalable to large problems

[[Fast Yet Effective Machine Unlearning.pdf#page=1&selection=49,9,51,17|Fast Yet Effective Machine Unlearning, page 1]]

> Our method is quite efficient in comparison to the existing methods, works for multi-class unlearning, does not put any constraints on the original optimization mechanism or network design, and works well in both small and large- scale vision tasks

[[Fast Yet Effective Machine Unlearning.pdf#page=1&selection=51,19,55,18|Fast Yet Effective Machine Unlearning, page 1]]


> Source code: https://github.com/vikram2000b/Fast-Machine-Unlearning

[[Fast Yet Effective Machine Unlearning.pdf#page=1&selection=56,56,57,60|Fast Yet Effective Machine Unlearning, page 1]]

> How can this process be made more efficient? What are the challenges? How do we know that the model has actually unlearned those class/classes of data? How to ensure minimal effect on the overall accuracy of the model? These are some of the questions that have been asked and possible solutions have been explored in recent times [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].

[[Fast Yet Effective Machine Unlearning.pdf#page=1&selection=118,0,124,64|Fast Yet Effective Machine Unlearning, page 1]]

> This is due to several complexities that arise while working with deep learning models. For example, the non- convex loss space [21] of CNNs makes it difficult to assess the effect of a data sample on the optimization trajectory and the final network weight combination. 

[[Fast Yet Effective Machine Unlearning.pdf#page=1&selection=149,10,153,38|Fast Yet Effective Machine Unlearning, page 1]]

> esearchers have [22] used influence functions to study the behaviour of black-box models such as CNNs through the lens of training samples. It is observed that data points with high training loss are more influential for the model parameters.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=4,11,24,17|Fast Yet Effective Machine Unlearning, page 2]]

> Recently, Huang et al. [25] proposed to learn an error-minimizing noise to make training examples unlearnable for deep learning models. The idea is to add such noise to the image samples that fools the model in believing nothing is to be learned from those samples. If used in training, such images have no effect on the model.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=27,56,49,13|Fast Yet Effective Machine Unlearning, page 2]]


> We know that the original model is trained by minimizing the loss for all the classes. So intuitively, maximizing a noise with respect to the model loss only for the unlearning class will help us learn such patterns that help forgetting. It can also be viewed as learning anti-samples for a given class and use these anti-samples to damage the previously learned information.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=60,35,66,42|Fast Yet Effective Machine Unlearning, page 2]]

> In this paper, we propose a framework for unlearning in a zero-glance privacy setting, i.e. the model can not see the unlearning class of data. We learn an error-maximizing noise matrix consisting of highly influential points corresponding to the unlearning class. After that, we train the model using the noise matrix to update the network weights.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=66,43,76,30|Fast Yet Effective Machine Unlearning, page 2]]

> We introduce Unlearning by Selective Impair and Repair (UNSIR), a single-pass method to unlearn single/multiple classes of data in a deep model without requiring access to the data samples of the requested set of unlearning classes.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=76,31,84,19|Fast Yet Effective Machine Unlearning, page 2]]

> Our method can be directly applied on the already trained deep model to make it forget the information about the requested class of data - while at the same time retaining very close to the original accuracy of the model on the remaining tasks. In fact our method performs exceedingly well in both unlearning the requested classes and retaining the accuracy on the remaining classes

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=84,20,90,37|Fast Yet Effective Machine Unlearning, page 2]]


> . To the best of our knowledge, this is the first method to achieve efficient multi- class unlearning in deep networks not only for small-scale problems (10 classes) but also for large-scale vision problems (100 classes). Our method works with the stringent zero- glance setting where data samples of the requested unlearning class is either not available or can not be used. This makes our solution unique and practical for real-world application.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=90,37,104,61|Fast Yet Effective Machine Unlearning, page 2]]

> An important and realistic use-case of unlearning is face recognition. We show that our method can effectively make a trained model forget a single as well as multiple faces in a highly efficient manner, without glancing at the samples of the unlearning faces.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=105,0,109,21|Fast Yet Effective Machine Unlearning, page 2]]


> 1) We introduce the problem of unlearning in a zero-glance setting which is a stricter formalization compared to the existing settings and offers a prospect for higher-level of privacy guarantees. 2) We learn an error-maximizing noise for the respec- tive unlearning classes. UNSIR is proposed to perform single-pass impair and single-pass repair by using a very high learning rate. The impair step makes the network forget the unlearning data classes. The repair step stabilizes the network weights to better remember the remaining tasks. The combination of both the steps allows it to obtain excellent unlearning and retain accu- racy. 3) We show that along with a better privacy setting and offering multi-class unlearning, our method is also highly efficient. The multi-class unlearning is performed in a single impair-repair pass instead of sequentially unlearning individual classes. 4) The proposed method works on large-scale vision datasets with strong performance on different types of deep networks such as convolutional networks and Vision Transformers. Our method does not require any prior information related to process of original model training and it is easily applicable to a wide class of deep networks. Specifically, we show excellent unlearning re- sults on face recognition. To the best of our knowledge, it is the first machine unlearning method to demonstrate all the above characteristics together.

[[Fast Yet Effective Machine Unlearning.pdf#page=2&selection=111,0,181,39|Fast Yet Effective Machine Unlearning, page 2]]

































































































































































































































































